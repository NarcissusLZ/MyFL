import os
import glob
import pandas as pd
import numpy as np
import csv
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

# === è·¯å¾„é…ç½® ===
DATASET_ROOT = './datasets/opt'
OUTPUT_CSV = './datasets/iot23.csv'

# === é‡‡æ ·é…é¢ ===
# å»ºè®®ä¿æŒ 20ä¸‡ï¼Œè¶³å¤Ÿè®­ç»ƒä¸”é€Ÿåº¦å¿«
MAX_SAMPLES_PER_CLASS = 50000

# === æ˜ å°„è¡¨é…ç½® ===

# 1. åè®®æ˜ å°„
PROTO_MAP = {'tcp': 0, 'udp': 1, 'icmp': 2, 'ipv6-icmp': 3, 'igmp': 4, 'arp': 5}

# 2. è¿æ¥çŠ¶æ€æ˜ å°„
STATE_MAP = {
    'S0': 0, 'S1': 1, 'SF': 2, 'REJ': 3, 'S2': 4, 'S3': 5,
    'RSTO': 6, 'RSTR': 7, 'RSTOS0': 8, 'RSTRH': 9,
    'SH': 10, 'SHR': 11, 'OTH': 12
}

# 3. [æ–°å¢] å¸¸è§åº”ç”¨å±‚æœåŠ¡æ˜ å°„
# å³ä½¿æ˜¯åŠ å¯†æµé‡ï¼ŒZeeké€šå¸¸ä¹Ÿèƒ½è¯†åˆ«å‡ºæ˜¯SSLæˆ–SSH
SERVICE_MAP = {
    '-': 0, 'http': 1, 'dns': 2, 'ssh': 3, 'ssl': 4,
    'dhcp': 5, 'irc': 6, 'ftp': 7, 'pop3': 8
}

# 4. [æ–°å¢] History å­—ç¬¦é›† (TCP æ ‡å¿—ä½ç»Ÿè®¡)
# S=Syn, h=Syn+Ack, A=Ack, D=Data, F=Fin, R=Rst
HISTORY_CHARS = ['S', 'h', 'A', 'D', 'F', 'R']

# === ç‰¹å¾åˆ—å®šä¹‰ ===
# åŸºç¡€æ•°å€¼ + ç±»åˆ« + æ–°å¢çš„ç«¯å£ + å†å²ç»Ÿè®¡ + è¡ç”Ÿç‰¹å¾
FEATURE_COLS = [
                   'duration', 'orig_bytes', 'resp_bytes', 'missed_bytes',
                   'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes',
                   'proto', 'conn_state', 'service', 'resp_port',  # åŸºç¡€ç‰¹å¾
                   'avg_orig_ip_bytes', 'avg_resp_ip_bytes'  # è¡ç”Ÿç‰¹å¾
               ] + [f'hist_{c}' for c in HISTORY_CHARS]  # History ç»Ÿè®¡ç‰¹å¾

# Zeek åŸå§‹åˆ—å (ç”¨äºè¯»å–å¯¹é½)
COL_NAMES_23 = [
    'ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p',
    'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes',
    'conn_state', 'local_orig', 'local_resp', 'missed_bytes',
    'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes',
    'tunnel_parents', 'label', 'detailed-label'
]

# æ¶ˆé™¤ Pandas è­¦å‘Š
pd.set_option('future.no_silent_downcasting', True)


def get_label_id(label_str):
    """
    é²æ£’çš„æ ‡ç­¾åŒ¹é…é€»è¾‘
    """
    s = str(label_str).lower()
    if 'okiru' in s: return 4
    if 'ddos' in s: return 1
    if 'portscan' in s: return 2
    if 'c&c' in s or 'botnet' in s or 'heartbeat' in s: return 3
    if 'attack' in s or 'malware' in s or 'virus' in s: return 4
    if 'benign' in s: return 0
    if 'malicious' in s: return 4
    return 0


def clean_and_convert(df):
    if df.empty: return pd.DataFrame()

    # === 1. æ ‡ç­¾æå– (ä¼˜åŒ–ç‰ˆ) ===
    # å–æœ€å3åˆ—ï¼Œä½¿ç”¨å‘é‡åŒ–æ‹¼æ¥ (æ¯” apply å¿«)
    last_cols = df.iloc[:, -3:].astype(str)
    combined = last_cols.iloc[:, 0].str.cat([last_cols.iloc[:, 1], last_cols.iloc[:, 2]], sep=' ')
    df['label'] = combined.apply(get_label_id)

    # === 2. åŸºç¡€æ¸…æ´— ===
    # å°† '-' å’Œ '(empty)' æ›¿æ¢ä¸º 0 (å¯¹äº duration, å¡« 0 æ„å‘³ç€æçŸ­æˆ–æœªè®°å½•ï¼Œè¿™æ˜¯å¸¸ç”¨åšæ³•)
    df = df.replace({'-': 0, '(empty)': 0}).infer_objects(copy=False)

    # å¼ºåˆ¶è½¬æ¢åŸºç¡€æ•°å€¼åˆ—
    num_cols = ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes',
                'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes']
    for c in num_cols:
        # ä½¿ç”¨ pd.to_numeric å¤„ç†å¯èƒ½æ··å…¥çš„éæ•°å­—å­—ç¬¦
        df[c] = pd.to_numeric(df.get(c, 0), errors='coerce').fillna(0)

    # === 3. ç‰¹å¾å·¥ç¨‹ (è¡¥å…¨é—æ¼) ===

    # [æ–°å¢] Service æ˜ å°„ (ä¸åœ¨å­—å…¸é‡Œçš„å½’ä¸º 9-Other)
    df['service'] = df['service'].astype(str).str.lower().map(lambda x: SERVICE_MAP.get(x, 9))

    # [åŸæœ‰] Proto & State
    df['proto'] = df['proto'].astype(str).str.lower().map(lambda x: PROTO_MAP.get(x, 6))
    df['conn_state'] = df['conn_state'].astype(str).map(lambda x: STATE_MAP.get(x, 13))

    # [æ–°å¢] ç›®çš„ç«¯å£ (å…³é”®ç‰¹å¾)
    # id.resp_p å¯èƒ½æ˜¯ '-' æˆ–æ•°å­—ï¼Œå¼ºåˆ¶è½¬ float å†è½¬ int (ä¸ºäº†å®‰å…¨)
    df['resp_port'] = pd.to_numeric(df['id.resp_p'], errors='coerce').fillna(0)

    # [æ–°å¢] History å­—ç¬¦ä¸²ç»Ÿè®¡ (å‘é‡åŒ–è®¡ç®—)
    df['history'] = df['history'].astype(str)
    for char in HISTORY_CHARS:
        # è®¡ç®— S, h, A... åœ¨ history å­—ç¬¦ä¸²ä¸­å‡ºç°çš„æ¬¡æ•°
        # æ¯”å¦‚ "ShAdDaF" -> S:1, h:1, A:1, D:2 ...
        df[f'hist_{char}'] = df['history'].str.count(char)

    # [æ–°å¢] å¹³å‡åŒ…å¤§å° (é˜²æ­¢é™¤ä»¥0ï¼ŒåŠ ä¸€ä¸ªæå°å€¼)
    df['avg_orig_ip_bytes'] = df['orig_ip_bytes'] / (df['orig_pkts'] + 1e-5)
    df['avg_resp_ip_bytes'] = df['resp_ip_bytes'] / (df['resp_pkts'] + 1e-5)

    # === 4. æœ€ç»ˆè¾“å‡ºæ•´ç† ===
    target_cols = FEATURE_COLS + ['label']

    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨ (é˜²æ­¢æŸäº›ç½•è§æƒ…å†µåˆ—ä¸¢å¤±)
    for col in target_cols:
        if col not in df.columns:
            df[col] = 0

    df = df[target_cols]

    # ç»Ÿä¸€ç±»å‹: ç‰¹å¾ç”¨ float32 (çœå†…å­˜ä¸”å…¼å®¹æ€§å¥½), æ ‡ç­¾ç”¨ int8
    df = df.astype(np.float32)
    df['label'] = df['label'].astype(np.int8)

    return df


def process_file_with_quota(args):
    file_path, shared_counts, lock = args
    CHUNK_SIZE = 1_000_000
    final_dfs = []

    try:
        # ä½¿ç”¨ names=COL_NAMES_23 å¼ºåˆ¶å¯¹é½åˆ—å
        reader = pd.read_csv(
            file_path, sep='\t', comment='#', names=COL_NAMES_23,
            chunksize=CHUNK_SIZE,
            low_memory=False, quoting=csv.QUOTE_NONE, on_bad_lines='skip'
        )

        for chunk_df in reader:
            cleaned_chunk = clean_and_convert(chunk_df)
            if cleaned_chunk.empty: continue

            # åˆ†ç»„ç­›é€‰
            groups = cleaned_chunk.groupby('label')

            with lock:
                for label, group_df in groups:
                    label = int(label)
                    current_count = shared_counts.get(label, 0)

                    if current_count >= MAX_SAMPLES_PER_CLASS:
                        continue

                    needed = MAX_SAMPLES_PER_CLASS - current_count
                    to_take = group_df.iloc[:needed]

                    shared_counts[label] = current_count + len(to_take)
                    final_dfs.append(to_take)

        if not final_dfs: return None
        return pd.concat(final_dfs)

    except Exception as e:
        # å¯ä»¥åœ¨è¿™é‡Œ print(e) è°ƒè¯•ï¼Œä½†åœ¨å¤šè¿›ç¨‹ä¸­å¯èƒ½ä¼šä¹±åº
        return None


def main():
    if os.path.exists(OUTPUT_CSV): os.remove(OUTPUT_CSV)

    # é€’å½’æŸ¥æ‰¾ conn.log.labeled æ–‡ä»¶
    files = glob.glob(os.path.join(DATASET_ROOT, '**', 'conn.log.labeled'), recursive=True)
    if not files:
        print(f"âŒ æœªåœ¨ {DATASET_ROOT} æ‰¾åˆ° conn.log.labeled æ–‡ä»¶")
        return

    print(f"ğŸš€ å¯åŠ¨ IoT-23 æ•°æ®å¤„ç† (Full Features Version)")
    print(f"ğŸ“‹ ç‰¹å¾æ•°é‡: {len(FEATURE_COLS)} (å« History ç»Ÿè®¡ & Service)")
    print(f"ğŸ¯ æ¯ç±»é…é¢: {MAX_SAMPLES_PER_CLASS}")

    # å†™å…¥ CSV å¤´éƒ¨
    pd.DataFrame(columns=FEATURE_COLS + ['label']).to_csv(OUTPUT_CSV, index=False)

    manager = multiprocessing.Manager()
    shared_counts = manager.dict({0: 0, 1: 0, 2: 0, 3: 0, 4: 0})
    lock = manager.Lock()

    tasks = [(f, shared_counts, lock) for f in files]

    # æ ¹æ®ä½ çš„å†…å­˜å¤§å°è°ƒæ•´ max_workers (æ¨è 4-8)
    SAFE_WORKERS = 4

    with ProcessPoolExecutor(max_workers=SAFE_WORKERS) as executor:
        futures = {executor.submit(process_file_with_quota, task): task for task in tasks}

        pbar = tqdm(as_completed(futures), total=len(files), desc="Processing Files")

        for future in pbar:
            try:
                result_df = future.result()
                current_stats = dict(shared_counts)

                # åŠ¨æ€æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                pbar.set_postfix(
                    Benign=f"{current_stats[0] // 1000}k",
                    DDoS=f"{current_stats[1] // 1000}k",
                    PortScan=f"{current_stats[2] // 1000}k",
                    C_C=f"{current_stats[3]}",
                    Malware=f"{current_stats[4] // 1000}k"
                )

                if result_df is not None and not result_df.empty:
                    # è¿½åŠ å†™å…¥ CSV (ä¸å†™å¤´éƒ¨)
                    result_df.to_csv(OUTPUT_CSV, mode='a', header=False, index=False)
                    del result_df

            except Exception as e:
                pass

    print("\n" + "=" * 50)
    print(f"âœ… å¤„ç†å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³: {OUTPUT_CSV}")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {dict(shared_counts)}")
    print("=" * 50)


if __name__ == '__main__':
    multiprocessing.freeze_support()
    main()